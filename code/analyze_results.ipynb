{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gym.spaces import Box\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = np.vstack(batch.state)\n",
    "        action_batch = np.vstack(batch.action)\n",
    "        reward_batch = np.vstack(batch.reward)\n",
    "        next_state_batch = np.vstack(batch.next_state)\n",
    "        done_batch = np.vstack(batch.done)\n",
    "        return state_batch, action_batch, next_state_batch, reward_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_function_on_action(centroid_locations, action, beta):\n",
    "    '''\n",
    "    centroid_locations: Tensor [batch x num_centroids (N) x a_dim (action_size)]\n",
    "    action_set: Tensor [batch x a_dim (action_size)]\n",
    "    beta: float\n",
    "        - Parameter for RBF function\n",
    "\n",
    "    Description: Computes the RBF function given centroid_locations and one action\n",
    "    '''\n",
    "    assert len(centroid_locations.shape) == 3, \"Must pass tensor with shape: [batch x N x a_dim]\"\n",
    "    assert len(action.shape) == 2, \"Must pass tensor with shape: [batch x a_dim]\"\n",
    "\n",
    "    diff_norm = centroid_locations - action.unsqueeze(dim=1).expand_as(centroid_locations)\n",
    "    diff_norm = diff_norm**2\n",
    "    diff_norm = torch.sum(diff_norm, dim=2)\n",
    "    diff_norm = torch.sqrt(diff_norm + 1e-7)\n",
    "    diff_norm = diff_norm * beta * -1\n",
    "    weights = F.softmax(diff_norm, dim=1)  # batch x N\n",
    "    return weights\n",
    "\n",
    "\n",
    "def rbf_function(centroid_locations, action_set, beta):\n",
    "\t'''\n",
    "\tcentroid_locations: Tensor [batch x num_centroids (N) x a_dim (action_size)]\n",
    "\taction_set: Tensor [batch x num_act x a_dim (action_size)]\n",
    "\t\t- Note: pass in num_act = 1 if you want a single action evaluated\n",
    "\tbeta: float\n",
    "\t\t- Parameter for RBF function\n",
    "\n",
    "\tDescription: Computes the RBF function given centroid_locations and some actions\n",
    "\t'''\n",
    "\tassert len(centroid_locations.shape) == 3, \"Must pass tensor with shape: [batch x N x a_dim]\"\n",
    "\tassert len(action_set.shape) == 3, \"Must pass tensor with shape: [batch x num_act x a_dim]\"\n",
    "\n",
    "\tdiff_norm = torch.cdist(centroid_locations, action_set, p=2)  # batch x N x num_act\n",
    "\tdiff_norm = diff_norm * beta * -1\n",
    "\tweights = F.softmax(diff_norm, dim=2)  # batch x N x num_act\n",
    "\treturn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\tModule that returns a view of the input which has a different size\n",
    "\tParameters:\n",
    "\t\t- args : Int...\n",
    "\t\t\tThe desired size\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, *args):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.shape = args\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\ts = self.__class__.__name__\n",
    "\t\ts += '{}'.format(self.shape)\n",
    "\t\treturn s\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(*self.shape)\n",
    "\n",
    "def sync_networks(target, online, alpha, copy=False):\n",
    "\tif copy == True:\n",
    "\t\tfor online_param, target_param in zip(online.parameters(), target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(online_param.data)\n",
    "\telif copy == False:\n",
    "\t\tfor online_param, target_param in zip(online.parameters(), target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(alpha * online_param.data +\n",
    "\t\t\t                        (1 - alpha) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params, env, state_size, action_size, device):\n",
    "        super(Net, self).__init__()\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.params = params\n",
    "        self.N = self.params['num_points']\n",
    "        self.max_a = self.env.action_space.high[0]\n",
    "        self.beta = self.params['temperature']\n",
    "\n",
    "        self.buffer_object = ReplayMemory(\n",
    "            capacity=self.params['max_buffer_size'])\n",
    "\n",
    "        self.state_size, self.action_size = state_size, action_size\n",
    "\n",
    "        self.value_module = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.params['layer_size']),\n",
    "            # Is ReLU a natural choice?\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.params['layer_size'], self.params['layer_size']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.params['layer_size'], self.params['layer_size']),\n",
    "            nn.ReLU(),\n",
    "            # Maybe a linear layer isn't ideal here\n",
    "            nn.Linear(self.params['layer_size'], self.N),\n",
    "        )\n",
    "\n",
    "        if self.params['num_layers_action_side'] == 1:\n",
    "            self.location_module = nn.Sequential(\n",
    "                nn.Linear(self.state_size, self.params['layer_size_action_side']),\n",
    "                nn.Dropout(p=self.params['dropout_rate']),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.params['layer_size_action_side'],\n",
    "                            self.action_size * self.N),\n",
    "                Reshape(-1, self.N, self.action_size),\n",
    "                # Change from tanh to sigmoid for custom action space\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        elif self.params['num_layers_action_side'] == 2:\n",
    "            self.location_module = nn.Sequential(\n",
    "                nn.Linear(self.state_size, self.params['layer_size_action_side']),\n",
    "                nn.Dropout(p=self.params['dropout_rate']),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.params['layer_size_action_side'],\n",
    "                          self.params['layer_size_action_side']),\n",
    "                nn.Dropout(p=self.params['dropout_rate']),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.params['layer_size_action_side'],\n",
    "                          self.action_size * self.N),\n",
    "                Reshape(-1, self.N, self.action_size),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        torch.nn.init.xavier_uniform_(self.location_module[0].weight)\n",
    "        torch.nn.init.zeros_(self.location_module[0].bias)\n",
    "\n",
    "        self.location_module[3].weight.data.uniform_(-.1, .1)\n",
    "        self.location_module[3].bias.data.uniform_(-1., 1.)\n",
    "\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        self.params_dic = [{\n",
    "            'params': self.value_module.parameters(), 'lr': self.params['learning_rate']\n",
    "        },\n",
    "                            {\n",
    "                                'params': self.location_module.parameters(),\n",
    "                                'lr': self.params['learning_rate_location_side']\n",
    "                            }]\n",
    "        try:\n",
    "            if self.params['optimizer'] == 'RMSprop':\n",
    "                self.optimizer = optim.RMSprop(self.params_dic)\n",
    "            elif self.params['optimizer'] == 'Adam':\n",
    "                self.optimizer = optim.Adam(self.params_dic)\n",
    "            else:\n",
    "                print('unknown optimizer ....')\n",
    "        except:\n",
    "            print(\"no optimizer specified ... \")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def get_centroid_values(self, s):\n",
    "        '''\n",
    "        given a batch of s, get all centroid values, [batch x N]\n",
    "        '''\n",
    "        centroid_values = self.value_module(s)\n",
    "        return centroid_values\n",
    "\n",
    "    def get_centroid_locations(self, s):\n",
    "        '''\n",
    "        given a batch of s, get all centroid_locations, [batch x N x a_dim]\n",
    "        '''\n",
    "        centroid_locations = self.max_a * self.location_module(s)\n",
    "        return centroid_locations\n",
    "\n",
    "    def get_best_qvalue_and_action(self, s):\n",
    "        '''\n",
    "        given a batch of states s, return Q(s,a), max_{a} ([batch x 1], [batch x a_dim])\n",
    "        '''\n",
    "        all_centroids = self.get_centroid_locations(s)\n",
    "        values = self.get_centroid_values(s)\n",
    "        weights = rbf_function(all_centroids, all_centroids, self.beta)  # [batch x N x N]\n",
    "        allq = torch.bmm(weights, values.unsqueeze(2)).squeeze(2)  # bs x num_centroids\n",
    "        # a -> all_centroids[idx] such that idx is max(dim=1) in allq\n",
    "        # a = torch.gather(all_centroids, dim=1, index=indices)\n",
    "        # (dim: bs x 1, dim: bs x action_dim)\n",
    "        best, indices = allq.max(dim=1)\n",
    "        if s.shape[0] == 1:\n",
    "            index_star = indices.item()\n",
    "            a = all_centroids[0, index_star]\n",
    "            return best, a\n",
    "        else:\n",
    "            return best, None\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        '''\n",
    "        given a batch of s, a, compute Q(s,a) [batch x 1]\n",
    "        '''\n",
    "        centroid_values = self.get_centroid_values(s)  # [batch_dim x N]\n",
    "        centroid_locations = self.get_centroid_locations(s)\n",
    "        # [batch x N]\n",
    "        centroid_weights = rbf_function_on_action(centroid_locations, a, self.beta)\n",
    "        output = torch.mul(centroid_weights, centroid_values)  # [batch x N]\n",
    "        output = output.sum(1, keepdim=True)  # [batch x 1]\n",
    "        return output\n",
    "\n",
    "    def e_greedy_policy(self, s, episode, train_or_test):\n",
    "        '''\n",
    "        Given state s, at episode, take random action with p=eps if training\n",
    "        Note - epsilon is determined by episode\n",
    "        '''\n",
    "        epsilon = 1.0 / np.power(episode, 1.0 / self.params['policy_parameter'])\n",
    "        if train_or_test == 'train' and random.random() < epsilon:\n",
    "            a = self.env.action_space.sample()\n",
    "            return a.tolist()\n",
    "        else:\n",
    "            self.eval()\n",
    "            s_matrix = np.array(s).reshape(1, self.state_size)\n",
    "            with torch.no_grad():\n",
    "                s = torch.from_numpy(s_matrix).float().to(self.device)\n",
    "                _, a = self.get_best_qvalue_and_action(s)\n",
    "                a = a.cpu().numpy()\n",
    "            self.train()\n",
    "            return a\n",
    "\n",
    "    def update(self, target_Q):\n",
    "        if len(self.buffer_object) < self.params['batch_size']:\n",
    "            return 0\n",
    "        s_matrix, a_matrix, sp_matrix, r_matrix, d_matrix = self.buffer_object.sample(self.params['batch_size'])\n",
    "        #r_matrix = np.clip(r_matrix,\n",
    "        #                    a_min=-self.params['reward_clip'],\n",
    "        #                    a_max=self.params['reward_clip'])\n",
    "\n",
    "        s_matrix = torch.from_numpy(s_matrix).float().to(self.device)\n",
    "        a_matrix = torch.from_numpy(a_matrix).float().to(self.device)\n",
    "        r_matrix = torch.from_numpy(r_matrix).float().to(self.device)\n",
    "        sp_matrix = torch.from_numpy(sp_matrix).float().to(self.device)\n",
    "        d_matrix = torch.from_numpy(d_matrix).float().to(self.device)\n",
    "\n",
    "        Q_star, _ = target_Q.get_best_qvalue_and_action(sp_matrix)\n",
    "        Q_star = Q_star.reshape((self.params['batch_size'], -1))\n",
    "        with torch.no_grad():\n",
    "            y = r_matrix + self.params['gamma'] * Q_star\n",
    "        y_hat = self.forward(s_matrix, a_matrix)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        #self.zero_grad()\n",
    "        sync_networks(\n",
    "            target=target_Q,\n",
    "            online=self,\n",
    "            alpha=self.params['target_network_learning_rate'],\n",
    "            copy=False)\n",
    "        return loss.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'env_name': 'solow',\n",
    "          'env': env,\n",
    "          'max_episode': 300,\n",
    "          'num_layers': 2,\n",
    "          'layer_size': 256,\n",
    "          'num_layers_action_side': 1,\n",
    "          'layer_size_action_side': 256,\n",
    "          'learning_rate': 0.00025,\n",
    "          'learning_rate_location_side': 2.5e-05,\n",
    "          'target_network_learning_rate': 0.005,\n",
    "          'max_buffer_size': 50000,\n",
    "          'gamma': env.β,\n",
    "          'batch_size': 256,\n",
    "          'num_points': 100,\n",
    "          'temperature': 1,\n",
    "          'policy_parameter': 1.5, # Determines how fast exploration decays\n",
    "          'norm_smoothing': 1e-05,\n",
    "          'updates_per_episode': 500,\n",
    "          'burn_steps': 15,\n",
    "          'dropout_rate': 0,\n",
    "          'optimizer': 'Adam',\n",
    "          'policy_type': 'e_greedy',\n",
    "          'seed_number': 2056}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44097944, 0.22297295])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GrowthModel:\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                δ=0.1,        # capital depreciation\n",
    "                β=0.96,       # discount factor\n",
    "                μ=0,          # shock location parameter\n",
    "                k_max=4,      # max capital\n",
    "                s=0.1,        # shock scale parameter\n",
    "                c_high=1,     # consumption bound\n",
    "                ):\n",
    "\n",
    "        self.u, self.f, self.β, self.μ, self.s, self.k_max, self.δ = u, f, β, μ, s, k_max, δ\n",
    "        # Use gym action space to accomodate continuous actions\n",
    "        self.action_space = Box(low=0, high=c_high, shape=(1, ), dtype=np.float32)\n",
    "        # What if, isntead of using percentage, we use number (bounding max consumption by 3)\n",
    "        # self.action_space = Box(low=0, high=3, shape=(1, ), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the current_state variable.\n",
    "        \"\"\"\n",
    "        k_init = np.random.beta(5, 5)\n",
    "        shock = self.μ + self.s*np.random.randn()\n",
    "        self.current_state = np.array([k_init, shock])\n",
    "        self.count = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action: float):\n",
    "        \"\"\"\n",
    "        Takes a step in the environment by sampling from the\n",
    "        transition matrix self.T given an action and the current_state and return the index of the next state.\n",
    "        It returns a tuple of (next_state, reward, terminal, truncation, flags).\n",
    "        \"\"\"\n",
    "        assert self.current_state is not None, \"State has not been reset\"\n",
    "        terminal, truncation = False, False\n",
    "        # Current period states\n",
    "        k, shock = self.current_state\n",
    "        # Compute macro variables\n",
    "        y = np.exp(shock)*self.f(k)\n",
    "        c = action[0]*y\n",
    "        reward = self.u(c)\n",
    "        # Next period states\n",
    "        next_k = y - c + (1 - self.δ)*k\n",
    "        shock = self.μ + self.s*np.random.randn()\n",
    "        self.current_state = np.array([next_k, shock])\n",
    "        self.count += 1\n",
    "        if self.count >= 1000:\n",
    "            truncation = True\n",
    "        if next_k == 0:\n",
    "            terminal = True\n",
    "        return self.current_state, reward, terminal, truncation\n",
    "\n",
    "    @property\n",
    "    def features_size(self):\n",
    "        return self.phi(0).shape[0]\n",
    "α = 0.4\n",
    "def fcd(k):\n",
    "    return np.nan_to_num(np.power(k, α), nan=-np.inf)\n",
    "\n",
    "env = GrowthModel(u=np.log, f=fcd, δ=0.1)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n",
      "n = 50, value SSE = 0.0002969293106994044, action SSE = 0.21659845113754272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8d/sm7kcrm54cd6hg3rnqsh6sxm0000gp/T/ipykernel_1384/607774263.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ks_tensor = torch.tensor(s, device=device, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100, value SSE = 0.21230601990407355, action SSE = 0.3093411922454834\n",
      "n = 150, value SSE = 0.23854210510424165, action SSE = 0.013707506470382214\n",
      "n = 200, value SSE = 0.24120404772592116, action SSE = 0.05019858479499817\n",
      "n = 250, value SSE = 0.25456966184704716, action SSE = 0.026028042659163475\n",
      "n = 300, value SSE = 0.22402535847422606, action SSE = 0.0440058708190918\n",
      "n = 350, value SSE = 0.2617604182302153, action SSE = 0.02421414665877819\n",
      "n = 400, value SSE = 0.20410422349822388, action SSE = 0.043907564133405685\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")  \n",
    "\n",
    "params = {'env_name': 'solow',\n",
    "          'env': env,\n",
    "          'max_episode': 300,\n",
    "          'num_layers': 2,\n",
    "          'layer_size': 256,\n",
    "          'num_layers_action_side': 1,\n",
    "          'layer_size_action_side': 256,\n",
    "          'learning_rate': 0.00025,\n",
    "          'learning_rate_location_side': 2.5e-05,\n",
    "          'target_network_learning_rate': 0.005,\n",
    "          'max_buffer_size': 50000,\n",
    "          'gamma': env.β,\n",
    "          'batch_size': 256,\n",
    "          'num_points': 100,\n",
    "          'temperature': 1,\n",
    "          'policy_parameter': 1.5, # Determines how fast exploration decays\n",
    "          'norm_smoothing': 1e-05,\n",
    "          'updates_per_episode': 500,\n",
    "          'burn_steps': 15,\n",
    "          'dropout_rate': 0,\n",
    "          'optimizer': 'Adam',\n",
    "          'policy_type': 'e_greedy',\n",
    "          'seed_number': 2056}\n",
    "s0 = env.reset()\n",
    "ks = np.linspace(0.1, 0.5, 400)\n",
    "s = np.vstack([ks, np.zeros_like(ks)]).T\n",
    "vstar = v_star(ks, α, env.β, env.μ)\n",
    "\n",
    "for n in np.arange(start=50, stop=401, step=50):\n",
    "    params['num_points'] = n\n",
    "    Q_object_target = Net(params,\n",
    "                            env,\n",
    "                            state_size=len(s0),\n",
    "                            action_size=len(env.action_space.low),\n",
    "                            device=device)\n",
    "    Q_object_target.load_state_dict(torch.load(f'models/model_{n}_1_299', map_location=torch.device('cpu')))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ks_tensor = torch.tensor(s, device=device, dtype=torch.float)\n",
    "        v, _ = Q_object_target.get_best_qvalue_and_action(ks_tensor)\n",
    "        v = v.detach().cpu().numpy()\n",
    "    actions = []\n",
    "    for i in ks:\n",
    "        s = torch.tensor([i, 0], device=device, dtype=torch.float).reshape(1, 2)\n",
    "        _, a = Q_object_target.get_best_qvalue_and_action(s)\n",
    "        actions.append(a.cpu().detach().numpy())\n",
    "    actions = np.array(actions).reshape(ks.shape)\n",
    "    print(f\"n = {n}, value SSE = {np.mean((v - vstar)**2)}, action SSE = {np.sum((actions - (1 - α * env.β))**2)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_star(k, α, β, μ):\n",
    "    \"\"\"\n",
    "    True value function\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - α * β) / (1 - β)\n",
    "    c2 = (μ + α * np.log(α * β)) / (1 - α)\n",
    "    c3 = 1 / (1 - β)\n",
    "    c4 = 1 / (1 - α * β)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * α * np.log(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25022193171840845"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = np.linspace(0.1, 0.5, 400)\n",
    "s = np.vstack([ks, np.zeros_like(ks)]).T\n",
    "with torch.no_grad():\n",
    "    ks_tensor = torch.tensor(s, device=device, dtype=torch.float)\n",
    "    v, _ = Q_object_target.get_best_qvalue_and_action(ks_tensor)\n",
    "    v = v.detach().cpu().numpy()\n",
    "vstar = v_star(ks, α, env.β, env.μ)\n",
    "np.sum((v - vstar) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024214147"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = []\n",
    "for i in ks:\n",
    "    s = torch.tensor([i, 0], device=device, dtype=torch.float).reshape(1, 2)\n",
    "    _, a = Q_object_target.get_best_qvalue_and_action(s)\n",
    "    actions.append(a.cpu().detach().numpy())\n",
    "np.sum((np.array(actions).reshape(ks.shape) - (1 - α * env.β)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
